set.seed(666);
train = sample(1:nrow(OJ), 2*nrow(OJ)/3);
corVals=round(abs(cor(OJ[,-c(1,14)])),digits = 3);
pairs(OJ);
OJ$StoreID=as.factor(OJ$StoreID);
OJ=OJ[,-c(6,7,13,14,15,16,17,18)];
linOJ=lm(PriceMM~.,data = OJ,subset=train);
linOJ$coefficients;
linOJpred=predict(linOJ,OJ)[-train];
plot(OJ$PriceMM[-train],linOJpred,main= "Figure 1: Linear Predictions");
mean((OJ$PriceMM[-train]-linOJpred)^2);
set.seed(666);
ojX=model.matrix(OJ$PriceMM~.,data= OJ);
ojY=OJ$PriceMM;
cv.out =cv.glmnet(ojX[train,], ojY[train], alpha =1, nfolds=10);
bestlam =cv.out$lambda.min;
bestlam;
plot(cv.out);
grid =10^seq(10,-2, length =100);
out1=glmnet(ojX,ojY,alpha=1,lambda = grid);
predict(out1,type ="coefficients",s=bestlam)[1:14,]
out2=glmnet(ojX[train,],ojY[train],alpha =1, lambda =grid);
lassoOJpred=predict(out2,s=bestlam,newx = ojX[-train,])
mean((OJ$PriceMM[-train]-lassoOJpred)^2);
gamOJ=gam(PriceMM~s(WeekofPurchase,3)+s(PriceCH,3)
+s(SalePriceMM,3)+s(SalePriceCH,3),
data = OJ,subset=train);
par(mfrow=c(2,2));
plot(gamOJ, se=TRUE);
gamOJpred=predict(gamOJ,newdata = OJ);
mean(((OJ$PriceMM-gamOJpred)[-train])^2);
treeOJ=tree(OJ$PriceMM~.,data = OJ,subset=train);
treeOJcv=cv.tree(treeOJ);
par(mfrow=c(1,1));
plot(treeOJcv$size,treeOJcv$dev,type = 's',
main="Figure 4: Tree Size vs Deviance");
plot(treeOJ);       text(treeOJ,pretty = 0,cex=0.8);
treeOJPred=predict(treeOJ,newdata=OJ);
mean(((treeOJPred-OJ$PriceMM)[-train])^2);
par(mfrow=c(2,2));
plot(OJ$PriceMM[-train],linOJpred);
plot(OJ$PriceMM[-train],lassoOJpred);
plot(OJ$PriceMM[-train],gamOJpred[-train]);
plot(OJ$PriceMM[-train],treeOJPred[-train]);
treeOJPred[-train];
treeOJPred
treeOJPred[-train];
gamOJpred
gamOJpred[-train];
OJ$PriceMM;
OJ$PriceMM[-train];
linOJpred;
rm(list=ls());
library(ISLR);
library(xtable);      ## contains 'xtable'
library(boot);        ## contains 'cv.glm'
attach(Auto);
set.seed(1);
train=sample(392,196);
lm.fit=lm(mpg~horsepower, data = Auto, subset = train);
mean((mpg-predict(lm.fit, Auto))[-train]^2);
predict(lm.fit,Auto);
predict(lm.fit,Auto)[-train];
rm(list=ls());
##Begun: 4-16-16
##Andrew Pangia
##MATH 805 Project Code
##general libraries
library(ISLR);
library(MASS);
library(gtools);
library(xtable);
##LASSO methods
library(glmnet);
##spline libraries
library(splines);
library(gam);
##this is used for subset selection, which may not be useful
##library(leaps);
##tree libraries
library(tree);
library(randomForest);
##due to the fact that we will be applying statistical learning
##methods which involve randomisation, we set the seed so that we can
##reproduce the results if necessary
set.seed(666);
##we arbitrarily split the dataset into a training set and a test set
##where the training set is twice the size of the test set
train = sample(1:nrow(OJ), 2*nrow(OJ)/3);
##We begin by scanning the pairs() function and checking correlated
##variables; the variables which are correlated are then removed from
##the data set;
##we temporarily remove Purchase (column 1) and Store7 (column 14)
corVals=round(abs(cor(OJ[,-c(1,14)])),digits = 3);
pairs(OJ);
##Note that StoreID is categorical, so we first set it as such
OJ$StoreID=as.factor(OJ$StoreID);
##We remove DiscCH (column 6), DiscMM (column 7), PriceDiff (column 13),
##Store7 (column 14), PctDiscMM (column 15), PctDiscCH(column 16),
##ListPriceDiff (column 17), and STORE (column 18).
OJ=OJ[,-c(6,7,13,14,15,16,17,18)];
#########################################################################
##we next run a multilinear regression model, predicting priceMM based on
##the other variables
linOJ=lm(PriceMM~.,data = OJ,subset=train);
linOJ$coefficients;
## (Intercept)    &    0.037255545\\
##  PurchaseMM    &    -0.011872255\\
##  WeekofPurchase  &  0.003096867\\
##  StoreID2     &    0.006077464\\
##  StoreID3     &     0.016076562\\
##  StoreID4     & -0.013164020\\
##  StoreID7     &  0.034618225\\
##  PriceCH      &   0.107738690\\
##  SpecialCH    &    0.048167066\\
##  SpecialMM    &    0.073316567\\
##  LoyalCH      &     -0.021447562\\
##  SalePriceMM  &    0.273008997\\
##  SalePriceCH  &     0.279584803
##predict the test values by predicting in general and then only
##considering the test values
linOJpred=predict(linOJ,OJ)[-train];
##we plot the test data versus the model's predictions
plot(OJ$PriceMM[-train],linOJpred,main= "Figure 1: Linear Predictions");
mean((OJ$PriceMM[-train]-linOJpred)^2);
##0.0066447
######################################################################
##having created a linear regression model, we now apply the LASSO method
##to remove variables and see how the accuracy of the model changes along
##with the interpretability of said model
##we minimise lambda in 10-fold CV
set.seed(666);
ojX=model.matrix(OJ$PriceMM~.,data= OJ);
ojY=OJ$PriceMM;
cv.out =cv.glmnet(ojX[train,], ojY[train], alpha =1, nfolds=10);
bestlam =cv.out$lambda.min;
bestlam;
plot(cv.out);
##best lambda value is 0.0001683695; we now predict the effective
##coefficients
grid =10^seq(10,-2, length =100);
out1=glmnet(ojX,ojY,alpha=1,lambda = grid);
predict(out1,type ="coefficients",s=bestlam)[1:14,]
##(Intercept)   &     0.40380163\\
##PurchaseMM    &    0\\
##WeekofPurchase &   0.00275528\\
##StoreID2      &     0\\
##StoreID3     &     0\\
##StoreID4    &      0\\
##StoreID7     &     0\\
##PriceCH     &      0.24049018\\
##SpecialCH    &     0\\
##SpecialMM    &     0\\
##LoyalCH      &     0\\
##SalePriceMM  &     0.19765726\\
##SalePriceCH  &     0.07920893
out2=glmnet(ojX[train,],ojY[train],alpha =1, lambda =grid);
lassoOJpred=predict(out2,s=bestlam,newx = ojX[-train,])
##the new test MSE
mean((OJ$PriceMM[-train]-lassoOJpred)^2);
##0.00727067; larger but uses only four variables
###################################################################
##we then apply a GAM with smoothing cubic splines on each variable
##from the LASSO method;
gamOJ=gam(PriceMM~s(WeekofPurchase,3)+s(PriceCH,3)
+s(SalePriceMM,3)+s(SalePriceCH,3),
data = OJ,subset=train);
##run all the different plots
par(mfrow=c(2,2));
plot(gamOJ, se=TRUE);
##predict the data
gamOJpred=predict(gamOJ,newdata = OJ);
##test MSE:
mean(((OJ$PriceMM-gamOJpred)[-train])^2);
##0.003040818; half the size of the linear
##finally, we apply a regression tree, using a simple tree for
##interpretability
treeOJ=tree(OJ$PriceMM~.,data = OJ,subset=train);
##we then cross-validate in order to see if pruning the tree helps
## at all
treeOJcv=cv.tree(treeOJ);
par(mfrow=c(1,1));
plot(treeOJcv$size,treeOJcv$dev,type = 's',
main="Figure 4: Tree Size vs Deviance");
##we don't prune the tree; bummer
plot(treeOJ);       text(treeOJ,pretty = 0,cex=0.8);
treeOJPred=predict(treeOJ,newdata=OJ);
##test MSE:
mean(((treeOJPred-OJ$PriceMM)[-train])^2);
##0.001945374 smallest
##compare predictions
par(mfrow=c(2,2));
plot(OJ$PriceMM[-train],linOJpred);
plot(OJ$PriceMM[-train],lassoOJpred);
plot(OJ$PriceMM[-train],gamOJpred[-train]);
plot(OJ$PriceMM[-train],treeOJPred[-train]);
par(mfrow=c(2,2));
plot(OJ$PriceMM[-train],linOJpred);
plot(OJ$PriceMM[-train],lassoOJpred);
plot(OJ$PriceMM[-train],gamOJpred[-train],ylab = "gamOJpred");
plot(OJ$PriceMM[-train],treeOJPred[-train],ylab = "treeOJPred");
library(ISLR);
library(MASS);
library(gtools);
library(xtable);
library(glmnet);
library(splines);
library(gam);
library(tree);
library(randomForest);
set.seed(666);
train = sample(1:nrow(OJ), 2*nrow(OJ)/3);
corVals=round(abs(cor(OJ[,-c(1,14)])),digits = 3);
OJ$StoreID=as.factor(OJ$StoreID);
OJ=OJ[,-c(6,7,13,14,15,16,17,18)];
linOJ=lm(PriceMM~.,data = OJ,subset=train);
linOJ$coefficients;
linOJpred=predict(linOJ,OJ)[-train];
plot(OJ$PriceMM[-train],linOJpred,main= "Figure 1: Linear Predictions");
plot(seq(1.6:2.4 by 0.001),seq(1.6:2.4 by 0.001),type="l",add=TRUE);
plot(seq(1.6,2.4, 0.001),seq(1.6,2.4,0.001),type="l",add=TRUE);
plot(seq(1.6,2.4, 0.001),seq(1.6,2.4,0.001),type="l",add=T);
warnings()
plot(seq(1.6,2.4, 0.001),seq(1.6,2.4,0.001),add=T,type="l");
plot(seq(1.6,2.4),seq(1.6,2.4),add=T,type="l");
plot(seq(1.6,2.4,0.01),seq(1.6,2.4,0.01),add=T,type="l");
plot(OJ$PriceMM[-train],linOJpred,main= "Figure 1: Linear Predictions");
plot(OJ$PriceMM[-train],linOJpred,main= "Figure 1: Linear Predictions",
add=T);
plot(seq(1.6,2.4,0.01),seq(1.6,2.4,0.01),type="l");
plot(OJ$PriceMM[-train],linOJpred,add=T,
main= "Figure 1: Linear Predictions");
lines(seq(1.6,2.4,0.01),seq(1.6,2.4,0.01),type="l");
warnings()
plot(OJ$PriceMM[-train],linOJpred,main= "Figure 1: Linear Predictions");
lines(seq(1.6,2.4,0.01),seq(1.6,2.4,0.01),type="l");
plot(OJ$PriceMM[-train],linOJpred,main= "Figure 1: Linear Predictions");
lines(seq(1.6,2.4,0.01),seq(1.6,2.4,0.01),type="l");
par(mfrow=c(2,2));
plot(OJ$PriceMM[-train],linOJpred);
lines(seq(1.6,2.4,0.01),seq(1.6,2.4,0.01),type="l");
plot(OJ$PriceMM[-train],lassoOJpred);
lines(seq(1.6,2.4,0.01),seq(1.6,2.4,0.01),type="l");
set.seed(666);
train = sample(1:nrow(OJ), 2*nrow(OJ)/3);
corVals=round(abs(cor(OJ[,-c(1,14)])),digits = 3);
rm(list = ls());
library(ISLR);
library(MASS);
library(gtools);
library(xtable);
library(glmnet);
library(splines);
library(gam);
library(tree);
library(randomForest);
set.seed(666);
train = sample(1:nrow(OJ), 2*nrow(OJ)/3);
corVals=round(abs(cor(OJ[,-c(1,14)])),digits = 3);
OJ$StoreID=as.factor(OJ$StoreID);
OJ=OJ[,-c(6,7,13,14,15,16,17,18)];
linOJ=lm(PriceMM~.,data = OJ,subset=train);
linOJ$coefficients;
linOJpred=predict(linOJ,OJ)[-train];
plot(OJ$PriceMM[-train],linOJpred,main= "Figure 1: Linear Predictions");
lines(seq(1.6,2.4,0.01),seq(1.6,2.4,0.01),type="l");
mean((OJ$PriceMM[-train]-linOJpred)^2);
set.seed(666);
ojX=model.matrix(OJ$PriceMM~.,data= OJ);
ojY=OJ$PriceMM;
cv.out =cv.glmnet(ojX[train,], ojY[train], alpha =1, nfolds=10);
bestlam =cv.out$lambda.min;
bestlam;
plot(cv.out);
grid =10^seq(10,-2, length =100);
out1=glmnet(ojX,ojY,alpha=1,lambda = grid);
predict(out1,type ="coefficients",s=bestlam)[1:14,]
out2=glmnet(ojX[train,],ojY[train],alpha =1, lambda =grid);
lassoOJpred=predict(out2,s=bestlam,newx = ojX[-train,])
mean((OJ$PriceMM[-train]-lassoOJpred)^2);
gamOJ=gam(PriceMM~s(WeekofPurchase,3)+s(PriceCH,3)
+s(SalePriceMM,3)+s(SalePriceCH,3),
data = OJ,subset=train);
par(mfrow=c(2,2));
plot(gamOJ, se=TRUE);
gamOJpred=predict(gamOJ,newdata = OJ);
mean(((OJ$PriceMM-gamOJpred)[-train])^2);
treeOJ=tree(OJ$PriceMM~.,data = OJ,subset=train);
treeOJcv=cv.tree(treeOJ);
par(mfrow=c(1,1));
plot(treeOJcv$size,treeOJcv$dev,type = 's',
main="Figure 4: Tree Size vs Deviance");
plot(treeOJ);       text(treeOJ,pretty = 0,cex=0.8);
treeOJPred=predict(treeOJ,newdata=OJ);
mean(((treeOJPred-OJ$PriceMM)[-train])^2);
par(mfrow=c(2,2));
plot(OJ$PriceMM[-train],linOJpred);
lines(seq(1.6,2.4,0.01),seq(1.6,2.4,0.01),type="l");
plot(OJ$PriceMM[-train],lassoOJpred);
lines(seq(1.6,2.4,0.01),seq(1.6,2.4,0.01),type="l");
plot(OJ$PriceMM[-train],gamOJpred[-train],ylab = "gamOJpred");
lines(seq(1.6,2.4,0.01),seq(1.6,2.4,0.01),type="l");
plot(OJ$PriceMM[-train],treeOJPred[-train],ylab = "treeOJPred");
lines(seq(1.6,2.4,0.01),seq(1.6,2.4,0.01),type="l");
library(ISLR);
library(ISLR);
fix(OJ);
min(OJ$WeekofPurchase);
max(OJ$WeekofPurchase);
library(ISLR);
library(MASS);
library(gtools);
library(xtable);
library(glmnet);
library(splines);
library(gam);
library(tree);
library(randomForest);
set.seed(666);
train = sample(1:nrow(OJ), 2*nrow(OJ)/3);
corVals=round(abs(cor(OJ[,-c(1,14)])),digits = 3);
OJ$StoreID=as.factor(OJ$StoreID);
OJ=OJ[,-c(6,7,13,14,15,16,17,18)];
summary(OJ);
xtable(summary(OJ));
set.seed(666);
linOJ=lm(PriceMM~.,data = OJ,subset=train);
linOJ$coefficients;
linOJpred=predict(linOJ,OJ)[-train];
plot(OJ$PriceMM[-train],linOJpred,main= "Figure 1: Linear Predictions");
lines(seq(1.6,2.4,0.01),seq(1.6,2.4,0.01),type="l");
mean((OJ$PriceMM[-train]-linOJpred)^2);
set.seed(666);
ojX=model.matrix(OJ$PriceMM~.,data= OJ);
ojY=OJ$PriceMM;
cv.out =cv.glmnet(ojX[train,], ojY[train], alpha =1, nfolds=10);
bestlam =cv.out$lambda.min;
bestlam;
plot(cv.out);
library(ISLR);
library(MASS);
library(gtools);
library(xtable);
##LASSO methods
library(glmnet);
##spline libraries
library(splines);
library(gam);
##this is used for subset selection, which may not be useful
##library(leaps);
##tree libraries
library(tree);
##due to the fact that we will be applying statistical learning
library(randomForest);
set.seed(666);
train = sample(1:nrow(OJ), 2*nrow(OJ)/3);
corVals=round(abs(cor(OJ[,-c(1,14)])),digits = 3);
pairs(OJ);
OJ$StoreID=as.factor(OJ$StoreID);
OJ=OJ[,-c(6,7,13,14,15,16,17,18)];
xtable(summary(OJ));
library(ISLR);
library(xtable);      ## contains 'xtable'
library(boot);        ## contains 'cv.glm'
attach(Auto);
set.seed(1);
source('~/Grad School/Clemson/Fall19/MATH 988/Hwk1/Hwk1Class.R', echo=TRUE)
rm(list=ls())
if(!require("magick"))
{
install.packages("magick",repos= "https://mirrors.nics.utk.edu/cran/");
}
install.packages("magick",repos= "https://mirrors.nics.utk.edu/cran/");
install.packages("magick", repos = "https://mirrors.nics.utk.edu/cran/")
library("magick");
x=4+3;
y=FALSE;
z=32;
print('check this');
help("install.packages")
if(!require("magick"))
{
install.packages("magick",repos= "https://mirrors.nics.utk.edu/cran/",
dependencies=TRUE,INSTALL_opts = c('--no-lock'));
}
install.packages("magick",repos= "https://mirrors.nics.utk.edu/cran/",
dependencies=TRUE,INSTALL_opts = c('--no-lock'));
install.packages("magick", repos = "https://mirrors.nics.utk.edu/cran/", dependencies = TRUE, INSTALL_opts = c("--no-lock"))
# Author: Andrew Pangia
# MATH 988
# Homework 1
#set the seed for replication
set.seed(5);
directory=dirname(parent.frame(2)$ofile);
#dirname(parent.frame(2)$ofile);
setwd(directory);
#2. Write a program that performs classification using any model
#     (some model of your own choosing)
#having read Chapter 4 of ISLR, it appears that a multi-variable logistic
#model will work best. This is done most efficiently by R's glm function.
#4. Use your classification program to classify images of cats and dogs
#The plan is to treat every pixel as a variable. As bad as it is, the
#different size of the photos will be accounted for by finding the smallest
#picture, and either cropping all other pictures into the same size,
#or attempting to magnify the smaller photos to a better size.
#Corrupted data will be handled by omitting it.
#to handle the images, the package 'magick' will be used
#this may not work
if(!require("magick"))
{
install.packages("magick");
}
library("magick");
#in case of an error, return null
gurkeIR<- function(image)
{
return(tryCatch(image_read(image),error=function(e) {} ));
}
#read the files out of their folders
catFiles<-list.files("Cats", pattern="*.jpg",full.names=TRUE);
catImages<-lapply(catFiles, gurkeIR);
corrupt<-lapply(catImages, is.null);
#cut out corrupt files if there are any of them
if(any(unlist(corrupt)))
{
catImages<-catImages[-which(unlist(corrupt))];
}
#repeat for dogs
dogFiles<-list.files("Dogs", pattern="*.jpg",full.names=TRUE);
dogImages<-lapply(dogFiles, gurkeIR);
corrupt<-lapply(dogImages, is.null);
if(any(unlist(corrupt)))
{
dogImages<-dogImages[-which(unlist(corrupt))];
}
#resize all the images; lapply doesn't work for some reason
#catImagesTest<-lapply(catImages, image_resize(), geometry=picSize)
picSize=geometry_size_pixels(width=150, height=150,
preserve_aspect = FALSE);
#convert the images into three-dimensional matrices; then convert these
#into vectors of average color values;
#the color vectors will all be added to a list of
#vectors, and another list will contain the labels 'cat' or 'dog';
#these two lists will be added to a dataframe, and that dataframe will
#then be murderized by the logistic method
#the list of vectors that act as the data
#dataList<-list();
nameList<-list();
redList<-list();
greenList<-list();
blueList<-list();
catIndex=1;
for (imageSlot in catImages)
{
#imageSlot
imageSlot=image_resize(imageSlot, picSize);
catTestMat=as.integer(image_data(
image=imageSlot, channel="RGB", frame=1));
redList[[catIndex]]=mean(as.vector(catTestMat[,,1]));
greenList[[catIndex]]=mean(as.vector(catTestMat[,,2]));
blueList[[catIndex]]=mean(as.vector(catTestMat[,,3]));
#dataList[[catIndex]]=as.vector(catTestMat);
nameList[[catIndex]]='cat';
catIndex=catIndex+1;
}
#back up one to point to the correct element of dataList
catIndex=catIndex-1;
dogIndex=1;
for (imageSlot in dogImages)
{
#imageSlot
imageSlot=image_resize(imageSlot, picSize);
dogTestMat=as.integer(image_data(
image=imageSlot, channel="RGB", frame=1));
redList[[catIndex+dogIndex]]=mean(as.vector(dogTestMat[,,1]));
greenList[[catIndex+dogIndex]]=mean(as.vector(dogTestMat[,,2]));
blueList[[catIndex+dogIndex]]=mean(as.vector(dogTestMat[,,3]));
#dataList[[catIndex]]=as.vector(catTestMat);
nameList[[catIndex+dogIndex]]='dog';
dogIndex=dogIndex+1;
}
avgRed<-unlist(redList);
avgGreen<-unlist(greenList);
avgBlue<-unlist(blueList);
name<-factor(unlist(nameList));
imagesFrame=data.frame(avgRed,avgGreen,avgBlue,name);
#write.csv(imagesFrame,"temp.csv")
#note that 0 represents cat, and 1 represents dog
#remove the categorical variable, which we know is at the end
#len=dim(imagesFrame)[2];
#design a training set of random values; take 75% of the values:
#runif is a Random UNIForm sampler
temp=runif(dim(imagesFrame)[1],0,1);
train=rep(FALSE,dim(imagesFrame)[1]);
train[temp<0.75]=TRUE;
#trainSet=imagesFrame[!!train,];
testSet=imagesFrame[!train,];
fit=glm(imagesFrame$name~.,data=imagesFrame,family=binomial, subset=train);
#look at colours
#set the predicted values
logitChance=predict(fit,testSet,type="response");
predictImages=rep("cat",dim(testSet)[1]);
#images closer to 1 are considered dogs
predictImages[logitChance>0.5]="dog";
ratioCorrect=mean(predictImages==testSet$name);
gurke=table(predictImages,testSet$name);
#new plan--too much data to use the bitmap, so get three variables
#avgRed, avgGreen, avgBlue; get these from summing over the vectors;
#
rm(list=ls())
clear
clc
